<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluating VLMs</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <h1><span class="pound">#</span>Evaluating VLMs</h1>
    <p class="date">Fri, May 10, 2024</p> 
    <a href="https://github.com/pythonlearner1025/reka-vibe-eval" style="margin-right: 10px">github</a>
    <a href="https://x.com/song_minjune/status/1788488665758630284">post</a>
    <p>Almost all of our interactions with the computer reflect as changes on the screen. All applications pass through the GUI at some point to reach us. Though each application injects their own DNA into their interfaces, they cannot discard design patterns that are universally human-understandable.</p>

    <p>Thanks to shared human-centricity across interfaces, we are able to use countless varied applications. A human programmer doesn't just interact with the code editor: he looks up errors on the browser, jots notes down in the notepad, and messages friends for help.</p>

    <p>For AI to be maximally helpful, it should be able to understand the user's goals and take action to help the user achieve them. There are two pieces to this puzzle: goal alignment and usefulness. First, the AI's prediction of what we want should be very close to what we actually want. Second, the AI should take actions to achieve our goals faster.</p>

    <p>In all environments, the AI should predict our intentions from observing our actions. The more information, the better, and generally images are more information dense than language. Currently, our most capable AI systems have advanced understanding of language, but poor understanding of images. Advancing visual capabilities is straightforward milestone for building maximally helpful agents that understand human goals and help achieve them.</p>

    <p>Important milestones for pushing capabilities in vision-language models (VLMs):</p>
    <ul>
        <li>Longer video understanding. current systems can handle around 127 imgs / 100,000 tokens or 1270/1mil tokens (gpt-4-turbo). at 0.1 fps (every 10 frames) around 3.52 hours of footage. This number should go up an order of magnitude more, around 24hrs/1mil tokens to start having agent that can help with longer term goals. We need to either 10x the compression, or 10x the context length.</li>
        <li>Smaller, VLMs running on-devise. Are you comfortable streaming your life, virtual and IRL, over https to some company?</li>
        <li>Faster VLMs, current SOTA (gpt-4-vision, claude-3-opus, reka-core) models take well over 30 seconds to begin streaming their first token. This is about 200x off from 150ms delay in in-person conversations.</li>
    </ul>

    <p>Of particular interest is smaller VLMs. They will always fall behind server-side VLMs in context-length and capability, but is a more viable pathway towards private, real-time assistants that can run on both mobile &amp; desktop.</p>

    <p>Advancement requires measurements of progress. The Reka AI team published a fantastic evaluation set of 269 high-quality visual question-answer pairs hand crafted and thoroughly vetted by their team. I evaluated 8 small VLMs in 1.8-8B parameters range on reka-vibe-eval, and share my results.</p>

    <h3>Reka-Vibe-Eval Score (%)</h3>
    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>hard</th>
          <th>normal</th>
          <th>all</th>
          <th>parameters</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Reka-flash</td>
          <td>39.2</td>
          <td>52.2</td>
          <td>59.9</td>
          <td>21B</td>
        </tr>
        <tr>
          <td>Claude-3-haiku-20240307</td>
          <td>38.5</td>
          <td>49.8</td>
          <td>56.4</td>
          <td>20B</td>
        </tr>
        <tr>
          <td>Reka Edge</td>
          <td>32.2</td>
          <td>53.1</td>
          <td>45.4</td>
          <td>7B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/Qwen/Qwen-VL-Chat">Qwen-VL-Chat</a></td>
          <td>30.87</td>
          <td>47.19</td>
          <td>41.2</td>
          <td>8B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/xtuner/llava-llama-3-8b">LLAVA-llama3-8b</a></td>
          <td>31.75</td>
          <td>45.86</td>
          <td>40.61</td>
          <td>8B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/qresearch/llama-3-vision-alpha">qresearch/llama-3-vision-alpha</a></td>
          <td>34.69</td>
          <td>43.49</td>
          <td>40.26</td>
          <td>8B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/BAAI/Bunny-Llama-3-8B-V">Bunny-8b-V-Instruct</a></td>
          <td>30.25</td>
          <td>42.31</td>
          <td>37.83</td>
          <td>8B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/Efficient-Large-Model/Llama-3-VILA1.5-8B?library=true">Llama3-VILA</a></td>
          <td>31</td>
          <td>41.42</td>
          <td>37.55</td>
          <td>8B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/MBZUAI/LLaVA-Phi-3-mini-4k-instruct">LLava-Phi-3-mini-4k-instruct</a></td>
          <td>29.75</td>
          <td>37.13</td>
          <td>34.39</td>
          <td>3.8B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/vikhyatk/moondream2">Moondream2</a></td>
          <td>24.24</td>
          <td>38.61</td>
          <td>33.3</td>
          <td>1.86B</td>
        </tr>
        <tr>
          <td><a href="https://huggingface.co/NousResearch/Nous-Hermes-2-Vision-Alpha">Nouse-Hermes2-Vision-Alpha</a></td>
          <td>25.51</td>
          <td>25.74</td>
          <td>25.66</td>
          <td>7B</td>
        </tr>
      </tbody>
    </table>
    
</body>
</html>